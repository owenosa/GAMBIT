{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be833239",
   "metadata": {},
   "source": [
    "W O R K O U T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aecb86a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Macro Pretraining...\n",
      "Macro Pretrain Epoch 1, Loss: 0.6770\n",
      "Macro Pretrain Epoch 2, Loss: 0.6768\n",
      "Macro Pretrain Epoch 3, Loss: 0.6773\n",
      "Macro Pretrain Epoch 4, Loss: 0.6768\n",
      "Macro Pretrain Epoch 5, Loss: 0.6770\n",
      "Macro Pretrain Epoch 6, Loss: 0.6764\n",
      "Macro Pretrain Epoch 7, Loss: 0.6766\n",
      "Macro Pretrain Epoch 8, Loss: 0.6772\n",
      "Macro Pretrain Epoch 9, Loss: 0.6772\n",
      "Macro Pretrain Epoch 10, Loss: 0.6762\n",
      "Macro Pretrain Epoch 11, Loss: 0.6769\n",
      "Macro Pretrain Epoch 12, Loss: 0.6766\n",
      "Macro Pretrain Epoch 13, Loss: 0.6766\n",
      "Macro Pretrain Epoch 14, Loss: 0.6768\n",
      "Macro Pretrain Epoch 15, Loss: 0.6772\n",
      "Macro Pretrain Epoch 16, Loss: 0.6766\n",
      "Macro Pretrain Epoch 17, Loss: 0.6765\n",
      "Macro Pretrain Epoch 18, Loss: 0.6769\n",
      "Macro Pretrain Epoch 19, Loss: 0.6764\n",
      "Macro Pretrain Epoch 20, Loss: 0.6770\n",
      "Macro Pretrain Epoch 21, Loss: 0.6766\n",
      "Macro Pretrain Epoch 22, Loss: 0.6768\n",
      "Macro Pretrain Epoch 23, Loss: 0.6772\n",
      "Macro Pretrain Epoch 24, Loss: 0.6763\n",
      "Macro Pretrain Epoch 25, Loss: 0.6768\n",
      "Macro Pretrain Epoch 26, Loss: 0.6766\n",
      "Macro Pretrain Epoch 27, Loss: 0.6766\n",
      "Macro Pretrain Epoch 28, Loss: 0.6769\n",
      "Macro Pretrain Epoch 29, Loss: 0.6763\n",
      "Macro Pretrain Epoch 30, Loss: 0.6766\n",
      "Macro Pretrain Epoch 31, Loss: 0.6766\n",
      "Macro Pretrain Epoch 32, Loss: 0.6768\n",
      "Macro Pretrain Epoch 33, Loss: 0.6769\n",
      "Macro Pretrain Epoch 34, Loss: 0.6761\n",
      "Macro Pretrain Epoch 35, Loss: 0.6771\n",
      "Macro Pretrain Epoch 36, Loss: 0.6768\n",
      "Macro Pretrain Epoch 37, Loss: 0.6773\n",
      "Macro Pretrain Epoch 38, Loss: 0.6768\n",
      "Macro Pretrain Epoch 39, Loss: 0.6770\n",
      "Macro Pretrain Epoch 40, Loss: 0.6776\n",
      "Macro Pretrain Epoch 41, Loss: 0.6765\n",
      "Macro Pretrain Epoch 42, Loss: 0.6772\n",
      "Macro Pretrain Epoch 43, Loss: 0.6768\n",
      "Macro Pretrain Epoch 44, Loss: 0.6767\n",
      "Macro Pretrain Epoch 45, Loss: 0.6770\n",
      "Macro Pretrain Epoch 46, Loss: 0.6769\n",
      "Macro Pretrain Epoch 47, Loss: 0.6768\n",
      "Macro Pretrain Epoch 48, Loss: 0.6768\n",
      "Macro Pretrain Epoch 49, Loss: 0.6775\n",
      "Macro Pretrain Epoch 50, Loss: 0.6768\n",
      "Macro Pretrain Epoch 51, Loss: 0.6768\n",
      "Macro Pretrain Epoch 52, Loss: 0.6768\n",
      "Macro Pretrain Epoch 53, Loss: 0.6768\n",
      "Macro Pretrain Epoch 54, Loss: 0.6754\n",
      "Macro Pretrain Epoch 55, Loss: 0.6775\n",
      "Macro Pretrain Epoch 56, Loss: 0.6766\n",
      "Macro Pretrain Epoch 57, Loss: 0.6766\n",
      "Macro Pretrain Epoch 58, Loss: 0.6772\n",
      "Macro Pretrain Epoch 59, Loss: 0.6768\n",
      "Macro Pretrain Epoch 60, Loss: 0.6765\n",
      "Macro Pretrain Epoch 61, Loss: 0.6769\n",
      "Macro Pretrain Epoch 62, Loss: 0.6767\n",
      "Macro Pretrain Epoch 63, Loss: 0.6767\n",
      "Macro Pretrain Epoch 64, Loss: 0.6769\n",
      "Macro Pretrain Epoch 65, Loss: 0.6762\n",
      "Macro Pretrain Epoch 66, Loss: 0.6769\n",
      "Macro Pretrain Epoch 67, Loss: 0.6763\n",
      "Macro Pretrain Epoch 68, Loss: 0.6766\n",
      "Macro Pretrain Epoch 69, Loss: 0.6764\n",
      "Macro Pretrain Epoch 70, Loss: 0.6770\n",
      "Macro Pretrain Epoch 71, Loss: 0.6766\n",
      "Macro Pretrain Epoch 72, Loss: 0.6769\n",
      "Macro Pretrain Epoch 73, Loss: 0.6772\n",
      "Macro Pretrain Epoch 74, Loss: 0.6786\n",
      "Macro Pretrain Epoch 75, Loss: 0.6773\n",
      "Macro Pretrain Epoch 76, Loss: 0.6769\n",
      "Macro Pretrain Epoch 77, Loss: 0.6767\n",
      "Macro Pretrain Epoch 78, Loss: 0.6772\n",
      "Macro Pretrain Epoch 79, Loss: 0.6772\n",
      "Macro Pretrain Epoch 80, Loss: 0.6766\n",
      "Macro Pretrain Epoch 81, Loss: 0.6767\n",
      "Macro Pretrain Epoch 82, Loss: 0.6770\n",
      "Macro Pretrain Epoch 83, Loss: 0.6766\n",
      "Macro Pretrain Epoch 84, Loss: 0.6770\n",
      "Macro Pretrain Epoch 85, Loss: 0.6765\n",
      "Macro Pretrain Epoch 86, Loss: 0.6769\n",
      "Macro Pretrain Epoch 87, Loss: 0.6768\n",
      "Macro Pretrain Epoch 88, Loss: 0.6769\n",
      "Macro Pretrain Epoch 89, Loss: 0.6774\n",
      "Macro Pretrain Epoch 90, Loss: 0.6772\n",
      "Macro Pretrain Epoch 91, Loss: 0.6768\n",
      "Macro Pretrain Epoch 92, Loss: 0.6769\n",
      "Macro Pretrain Epoch 93, Loss: 0.6768\n",
      "Macro Pretrain Epoch 94, Loss: 0.6770\n",
      "Macro Pretrain Epoch 95, Loss: 0.6769\n",
      "Macro Pretrain Epoch 96, Loss: 0.6766\n",
      "Macro Pretrain Epoch 97, Loss: 0.6766\n",
      "Macro Pretrain Epoch 98, Loss: 0.6767\n",
      "Macro Pretrain Epoch 99, Loss: 0.6771\n",
      "Macro Pretrain Epoch 100, Loss: 0.6770\n",
      "Starting Vol Pretraining...\n",
      "Vol Pretrain Epoch 1, Loss: 0.0764\n",
      "Vol Pretrain Epoch 2, Loss: 0.0462\n",
      "Vol Pretrain Epoch 3, Loss: 0.0442\n",
      "Vol Pretrain Epoch 4, Loss: 0.0440\n",
      "Vol Pretrain Epoch 5, Loss: 0.0440\n",
      "Vol Pretrain Epoch 6, Loss: 0.0439\n",
      "Vol Pretrain Epoch 7, Loss: 0.0439\n",
      "Vol Pretrain Epoch 8, Loss: 0.0439\n",
      "Vol Pretrain Epoch 9, Loss: 0.0439\n",
      "Vol Pretrain Epoch 10, Loss: 0.0439\n",
      "Vol Pretrain Epoch 11, Loss: 0.0439\n",
      "Vol Pretrain Epoch 12, Loss: 0.0439\n",
      "Vol Pretrain Epoch 13, Loss: 0.0439\n",
      "Vol Pretrain Epoch 14, Loss: 0.0440\n",
      "Vol Pretrain Epoch 15, Loss: 0.0440\n",
      "Vol Pretrain Epoch 16, Loss: 0.0439\n",
      "Vol Pretrain Epoch 17, Loss: 0.0439\n",
      "Vol Pretrain Epoch 18, Loss: 0.0440\n",
      "Vol Pretrain Epoch 19, Loss: 0.0439\n",
      "Vol Pretrain Epoch 20, Loss: 0.0440\n",
      "Vol Pretrain Epoch 21, Loss: 0.0439\n",
      "Vol Pretrain Epoch 22, Loss: 0.0439\n",
      "Vol Pretrain Epoch 23, Loss: 0.0440\n",
      "Vol Pretrain Epoch 24, Loss: 0.0440\n",
      "Vol Pretrain Epoch 25, Loss: 0.0440\n",
      "Vol Pretrain Epoch 26, Loss: 0.0439\n",
      "Vol Pretrain Epoch 27, Loss: 0.0439\n",
      "Vol Pretrain Epoch 28, Loss: 0.0439\n",
      "Vol Pretrain Epoch 29, Loss: 0.0439\n",
      "Vol Pretrain Epoch 30, Loss: 0.0439\n",
      "Vol Pretrain Epoch 31, Loss: 0.0439\n",
      "Vol Pretrain Epoch 32, Loss: 0.0439\n",
      "Vol Pretrain Epoch 33, Loss: 0.0439\n",
      "Vol Pretrain Epoch 34, Loss: 0.0440\n",
      "Vol Pretrain Epoch 35, Loss: 0.0440\n",
      "Vol Pretrain Epoch 36, Loss: 0.0440\n",
      "Vol Pretrain Epoch 37, Loss: 0.0440\n",
      "Vol Pretrain Epoch 38, Loss: 0.0439\n",
      "Vol Pretrain Epoch 39, Loss: 0.0440\n",
      "Vol Pretrain Epoch 40, Loss: 0.0440\n",
      "Vol Pretrain Epoch 41, Loss: 0.0439\n",
      "Vol Pretrain Epoch 42, Loss: 0.0439\n",
      "Vol Pretrain Epoch 43, Loss: 0.0439\n",
      "Vol Pretrain Epoch 44, Loss: 0.0440\n",
      "Vol Pretrain Epoch 45, Loss: 0.0439\n",
      "Vol Pretrain Epoch 46, Loss: 0.0440\n",
      "Vol Pretrain Epoch 47, Loss: 0.0440\n",
      "Vol Pretrain Epoch 48, Loss: 0.0440\n",
      "Vol Pretrain Epoch 49, Loss: 0.0439\n",
      "Vol Pretrain Epoch 50, Loss: 0.0439\n",
      "Vol Pretrain Epoch 51, Loss: 0.0439\n",
      "Vol Pretrain Epoch 52, Loss: 0.0439\n",
      "Vol Pretrain Epoch 53, Loss: 0.0440\n",
      "Vol Pretrain Epoch 54, Loss: 0.0440\n",
      "Vol Pretrain Epoch 55, Loss: 0.0439\n",
      "Vol Pretrain Epoch 56, Loss: 0.0440\n",
      "Vol Pretrain Epoch 57, Loss: 0.0439\n",
      "Vol Pretrain Epoch 58, Loss: 0.0440\n",
      "Vol Pretrain Epoch 59, Loss: 0.0439\n",
      "Vol Pretrain Epoch 60, Loss: 0.0440\n",
      "Vol Pretrain Epoch 61, Loss: 0.0440\n",
      "Vol Pretrain Epoch 62, Loss: 0.0439\n",
      "Vol Pretrain Epoch 63, Loss: 0.0439\n",
      "Vol Pretrain Epoch 64, Loss: 0.0439\n",
      "Vol Pretrain Epoch 65, Loss: 0.0440\n",
      "Vol Pretrain Epoch 66, Loss: 0.0440\n",
      "Vol Pretrain Epoch 67, Loss: 0.0439\n",
      "Vol Pretrain Epoch 68, Loss: 0.0439\n",
      "Vol Pretrain Epoch 69, Loss: 0.0440\n",
      "Vol Pretrain Epoch 70, Loss: 0.0439\n",
      "Vol Pretrain Epoch 71, Loss: 0.0440\n",
      "Vol Pretrain Epoch 72, Loss: 0.0439\n",
      "Vol Pretrain Epoch 73, Loss: 0.0439\n",
      "Vol Pretrain Epoch 74, Loss: 0.0439\n",
      "Vol Pretrain Epoch 75, Loss: 0.0440\n",
      "Vol Pretrain Epoch 76, Loss: 0.0440\n",
      "Vol Pretrain Epoch 77, Loss: 0.0439\n",
      "Vol Pretrain Epoch 78, Loss: 0.0439\n",
      "Vol Pretrain Epoch 79, Loss: 0.0439\n",
      "Vol Pretrain Epoch 80, Loss: 0.0440\n",
      "Vol Pretrain Epoch 81, Loss: 0.0440\n",
      "Vol Pretrain Epoch 82, Loss: 0.0440\n",
      "Vol Pretrain Epoch 83, Loss: 0.0439\n",
      "Vol Pretrain Epoch 84, Loss: 0.0439\n",
      "Vol Pretrain Epoch 85, Loss: 0.0439\n",
      "Vol Pretrain Epoch 86, Loss: 0.0439\n",
      "Vol Pretrain Epoch 87, Loss: 0.0439\n",
      "Vol Pretrain Epoch 88, Loss: 0.0439\n",
      "Vol Pretrain Epoch 89, Loss: 0.0439\n",
      "Vol Pretrain Epoch 90, Loss: 0.0440\n",
      "Vol Pretrain Epoch 91, Loss: 0.0439\n",
      "Vol Pretrain Epoch 92, Loss: 0.0439\n",
      "Vol Pretrain Epoch 93, Loss: 0.0439\n",
      "Vol Pretrain Epoch 94, Loss: 0.0439\n",
      "Vol Pretrain Epoch 95, Loss: 0.0439\n",
      "Vol Pretrain Epoch 96, Loss: 0.0440\n",
      "Vol Pretrain Epoch 97, Loss: 0.0439\n",
      "Vol Pretrain Epoch 98, Loss: 0.0440\n",
      "Vol Pretrain Epoch 99, Loss: 0.0439\n",
      "Vol Pretrain Epoch 100, Loss: 0.0439\n",
      "Starting Price Pretraining...\n",
      "Price Pretrain Epoch 1, Loss: 0.0003\n",
      "Price Pretrain Epoch 2, Loss: 0.0001\n",
      "Price Pretrain Epoch 3, Loss: 0.0001\n",
      "Price Pretrain Epoch 4, Loss: 0.0001\n",
      "Price Pretrain Epoch 5, Loss: 0.0001\n",
      "Price Pretrain Epoch 6, Loss: 0.0001\n",
      "Price Pretrain Epoch 7, Loss: 0.0001\n",
      "Price Pretrain Epoch 8, Loss: 0.0001\n",
      "Price Pretrain Epoch 9, Loss: 0.0001\n",
      "Price Pretrain Epoch 10, Loss: 0.0001\n",
      "Price Pretrain Epoch 11, Loss: 0.0001\n",
      "Price Pretrain Epoch 12, Loss: 0.0001\n",
      "Price Pretrain Epoch 13, Loss: 0.0001\n",
      "Price Pretrain Epoch 14, Loss: 0.0001\n",
      "Price Pretrain Epoch 15, Loss: 0.0001\n",
      "Price Pretrain Epoch 16, Loss: 0.0001\n",
      "Price Pretrain Epoch 17, Loss: 0.0001\n",
      "Price Pretrain Epoch 18, Loss: 0.0001\n",
      "Price Pretrain Epoch 19, Loss: 0.0001\n",
      "Price Pretrain Epoch 20, Loss: 0.0001\n",
      "Price Pretrain Epoch 21, Loss: 0.0001\n",
      "Price Pretrain Epoch 22, Loss: 0.0001\n",
      "Price Pretrain Epoch 23, Loss: 0.0001\n",
      "Price Pretrain Epoch 24, Loss: 0.0001\n",
      "Price Pretrain Epoch 25, Loss: 0.0001\n",
      "Price Pretrain Epoch 26, Loss: 0.0001\n",
      "Price Pretrain Epoch 27, Loss: 0.0001\n",
      "Price Pretrain Epoch 28, Loss: 0.0001\n",
      "Price Pretrain Epoch 29, Loss: 0.0001\n",
      "Price Pretrain Epoch 30, Loss: 0.0001\n",
      "Price Pretrain Epoch 31, Loss: 0.0001\n",
      "Price Pretrain Epoch 32, Loss: 0.0001\n",
      "Price Pretrain Epoch 33, Loss: 0.0001\n",
      "Price Pretrain Epoch 34, Loss: 0.0001\n",
      "Price Pretrain Epoch 35, Loss: 0.0001\n",
      "Price Pretrain Epoch 36, Loss: 0.0001\n",
      "Price Pretrain Epoch 37, Loss: 0.0001\n",
      "Price Pretrain Epoch 38, Loss: 0.0001\n",
      "Price Pretrain Epoch 39, Loss: 0.0001\n",
      "Price Pretrain Epoch 40, Loss: 0.0001\n",
      "Price Pretrain Epoch 41, Loss: 0.0001\n",
      "Price Pretrain Epoch 42, Loss: 0.0001\n",
      "Price Pretrain Epoch 43, Loss: 0.0001\n",
      "Price Pretrain Epoch 44, Loss: 0.0001\n",
      "Price Pretrain Epoch 45, Loss: 0.0001\n",
      "Price Pretrain Epoch 46, Loss: 0.0001\n",
      "Price Pretrain Epoch 47, Loss: 0.0001\n",
      "Price Pretrain Epoch 48, Loss: 0.0001\n",
      "Price Pretrain Epoch 49, Loss: 0.0001\n",
      "Price Pretrain Epoch 50, Loss: 0.0001\n",
      "Price Pretrain Epoch 51, Loss: 0.0001\n",
      "Price Pretrain Epoch 52, Loss: 0.0001\n",
      "Price Pretrain Epoch 53, Loss: 0.0001\n",
      "Price Pretrain Epoch 54, Loss: 0.0001\n",
      "Price Pretrain Epoch 55, Loss: 0.0001\n",
      "Price Pretrain Epoch 56, Loss: 0.0001\n",
      "Price Pretrain Epoch 57, Loss: 0.0001\n",
      "Price Pretrain Epoch 58, Loss: 0.0001\n",
      "Price Pretrain Epoch 59, Loss: 0.0001\n",
      "Price Pretrain Epoch 60, Loss: 0.0001\n",
      "Price Pretrain Epoch 61, Loss: 0.0001\n",
      "Price Pretrain Epoch 62, Loss: 0.0001\n",
      "Price Pretrain Epoch 63, Loss: 0.0001\n",
      "Price Pretrain Epoch 64, Loss: 0.0001\n",
      "Price Pretrain Epoch 65, Loss: 0.0001\n",
      "Price Pretrain Epoch 66, Loss: 0.0001\n",
      "Price Pretrain Epoch 67, Loss: 0.0001\n",
      "Price Pretrain Epoch 68, Loss: 0.0001\n",
      "Price Pretrain Epoch 69, Loss: 0.0001\n",
      "Price Pretrain Epoch 70, Loss: 0.0001\n",
      "Price Pretrain Epoch 71, Loss: 0.0001\n",
      "Price Pretrain Epoch 72, Loss: 0.0001\n",
      "Price Pretrain Epoch 73, Loss: 0.0001\n",
      "Price Pretrain Epoch 74, Loss: 0.0001\n",
      "Price Pretrain Epoch 75, Loss: 0.0001\n",
      "Price Pretrain Epoch 76, Loss: 0.0001\n",
      "Price Pretrain Epoch 77, Loss: 0.0001\n",
      "Price Pretrain Epoch 78, Loss: 0.0001\n",
      "Price Pretrain Epoch 79, Loss: 0.0001\n",
      "Price Pretrain Epoch 80, Loss: 0.0001\n",
      "Price Pretrain Epoch 81, Loss: 0.0001\n",
      "Price Pretrain Epoch 82, Loss: 0.0001\n",
      "Price Pretrain Epoch 83, Loss: 0.0001\n",
      "Price Pretrain Epoch 84, Loss: 0.0001\n",
      "Price Pretrain Epoch 85, Loss: 0.0001\n",
      "Price Pretrain Epoch 86, Loss: 0.0001\n",
      "Price Pretrain Epoch 87, Loss: 0.0001\n",
      "Price Pretrain Epoch 88, Loss: 0.0001\n",
      "Price Pretrain Epoch 89, Loss: 0.0001\n",
      "Price Pretrain Epoch 90, Loss: 0.0001\n",
      "Price Pretrain Epoch 91, Loss: 0.0001\n",
      "Price Pretrain Epoch 92, Loss: 0.0001\n",
      "Price Pretrain Epoch 93, Loss: 0.0001\n",
      "Price Pretrain Epoch 94, Loss: 0.0001\n",
      "Price Pretrain Epoch 95, Loss: 0.0001\n",
      "Price Pretrain Epoch 96, Loss: 0.0001\n",
      "Price Pretrain Epoch 97, Loss: 0.0001\n",
      "Price Pretrain Epoch 98, Loss: 0.0001\n",
      "Price Pretrain Epoch 99, Loss: 0.0001\n",
      "Price Pretrain Epoch 100, Loss: 0.0001\n",
      "Loading checkpoint from checkpoints\\es_agent.pth\n",
      "Checkpoint saved to checkpoints\\es_agent.pth\n"
     ]
    }
   ],
   "source": [
    "# Full pretrain + fine-tune pipeline\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from torch.utils.data import DataLoader\n",
    "from Agents.r1_es import (init_trading_agent, generate_labels,\n",
    "                          RLPretrainDataset, pretrain_macro, pretrain_vol, pretrain_price,\n",
    "                          unfreeze_all)\n",
    "\n",
    "from training_module import run_training\n",
    "from RL1 import load_rl_data, TradingEnv\n",
    "\n",
    "\n",
    "# === User-configurable parameters ===\n",
    "ticker = 'ES'\n",
    "rl_folder = 'RL Data'\n",
    "episodes_per_run = 1\n",
    "checkpoint_dir = 'checkpoints'\n",
    "batch_size = 32\n",
    "pretrain_epochs = 100\n",
    "\n",
    "# Ensure checkpoint directory exists\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "ckpt_path = os.path.join(checkpoint_dir, f'{ticker.lower()}_agent.pth')\n",
    "\n",
    "# === STEP 1: Load Data and Generate Labels ===\n",
    "df = pd.read_csv(os.path.join(rl_folder, f\"RL - {ticker}.csv\"))\n",
    "df = generate_labels(df)\n",
    "\n",
    "feature_cols = ['Macro', 'Vol', 'Security', 'Security Proba', 'AVWAP', 'Base', 'R1', 'R2', 'S1', 'S2']\n",
    "pretrain_feature_cols = feature_cols + ['Price']\n",
    "\n",
    "# === STEP 2: Build Pretraining Datasets ===\n",
    "macro_dataset = RLPretrainDataset(df, pretrain_feature_cols, label_type='direction')\n",
    "vol_dataset   = RLPretrainDataset(df, pretrain_feature_cols, label_type='size')\n",
    "price_dataset = RLPretrainDataset(df, pretrain_feature_cols, label_type='stop')\n",
    "\n",
    "macro_loader = DataLoader(macro_dataset, batch_size=batch_size, shuffle=True)\n",
    "vol_loader   = DataLoader(vol_dataset, batch_size=batch_size, shuffle=True)\n",
    "price_loader = DataLoader(price_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# === STEP 3: Initialize Agent ===\n",
    "agent = init_trading_agent(input_size=len(pretrain_feature_cols))\n",
    "\n",
    "# === STEP 4: Pretrain Each Branch ===\n",
    "print(\"Starting Macro Pretraining...\")\n",
    "pretrain_macro(agent, macro_loader, epochs=pretrain_epochs)\n",
    "\n",
    "print(\"Starting Vol Pretraining...\")\n",
    "pretrain_vol(agent, vol_loader, epochs=pretrain_epochs)\n",
    "\n",
    "print(\"Starting Price Pretraining...\")\n",
    "pretrain_price(agent, price_loader, epochs=pretrain_epochs)\n",
    "\n",
    "# === STEP 5: Unfreeze Agent for RL Fine-tuning ===\n",
    "unfreeze_all(agent)\n",
    "\n",
    "# === STEP 5.5: Rebuild agent for RL ===\n",
    "agent_rl = init_trading_agent(input_size=len(feature_cols) + 2)  # (Macro...S2 + Price + position_size)\n",
    "\n",
    "# Manually filter pretrained state_dict\n",
    "pretrained_state = agent.state_dict()\n",
    "filtered_state = {k: v for k, v in pretrained_state.items() if not k.startswith('shared_layer.0.')}\n",
    "\n",
    "# Load only compatible parameters\n",
    "agent_rl.load_state_dict(filtered_state, strict=False)\n",
    "\n",
    "# Replace agent\n",
    "agent = agent_rl\n",
    "\n",
    "\n",
    "# === STEP 6: Load Checkpoint if available ===\n",
    "if os.path.exists(ckpt_path):\n",
    "    print(f\"Loading checkpoint from {ckpt_path}\")\n",
    "    agent.load_state_dict(torch.load(ckpt_path))\n",
    "else:\n",
    "    print(\"No checkpoint found, starting from pretrained weights.\")\n",
    "\n",
    "# === STEP 7: Create TradingEnv ===\n",
    "df_env = load_rl_data(rl_folder, ticker)\n",
    "env = TradingEnv(df_env)\n",
    "\n",
    "# === STEP 8: Run RL Training ===\n",
    "results = run_training(env, agent, num_episodes=episodes_per_run)\n",
    "\n",
    "# === STEP 9: Save updated checkpoint ===\n",
    "torch.save(agent.state_dict(), ckpt_path)\n",
    "print(f\"Checkpoint saved to {ckpt_path}\")\n",
    "\n",
    "# === STEP 10: Save results ===\n",
    "results.to_csv('results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e18f59d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
